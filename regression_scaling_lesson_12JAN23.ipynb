{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8a652f",
   "metadata": {},
   "source": [
    "## SCALING LESSON\n",
    "12 January 2023  \n",
    "https://ds.codeup.com/regression/split-and-scale/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e52ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff66ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from env import sql_connexion\n",
    "import env\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import acquire\n",
    "import wrangle2\n",
    "\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09e34d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['propertylandusetypeid', 'Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train, validate, test \u001b[38;5;241m=\u001b[39m \u001b[43mwrangle2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrangle_zillow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/codeup-data-science/regression-exercises/wrangle2.py:74\u001b[0m, in \u001b[0;36mwrangle_zillow\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrangle_zillow\u001b[39m():\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    this function will acquire, prepare and split\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    the zillow data in one function.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     train, validate, test \u001b[38;5;241m=\u001b[39m \u001b[43mprep_zillow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_zillow_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train, validate, test\n",
      "File \u001b[0;32m~/Desktop/codeup-data-science/regression-exercises/wrangle2.py:54\u001b[0m, in \u001b[0;36mprep_zillow\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     47\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbedroomcnt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbedrooms\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     48\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbathroomcnt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbathrooms\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     49\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcalculatedfinishedsquarefeet\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     50\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaxvaluedollarcnt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtax_value\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     51\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myearbuilt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_built\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# dropping unnecessary columns 'propertylandusetypeid' and 'Unnamed: 0'\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpropertylandusetypeid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#drop null values- at most there were 9000 nulls (this is only 0.5% of 2.1M)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4957\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4809\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4811\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4818\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4819\u001b[0m ):\n\u001b[1;32m   4820\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4821\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4822\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4955\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4956\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4963\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4964\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4965\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:6661\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6661\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6662\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['propertylandusetypeid', 'Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train, validate, test = wrangle2.wrangle_zillow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f972e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(), validate.head(), test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3240368",
   "metadata": {},
   "source": [
    "## SCALING\n",
    "\n",
    "- No y_train, y_val : \n",
    "    - Do we need to scale our y-value ? No, because the scale keeps the same ration between the individual values, and the ratio will be transfered to the target. The scale between features & target remains the same.\n",
    "    - We're not using the y-value to train the model.  \n",
    "- We're concerned with the x-values (features) used to predict target.\n",
    "\n",
    "\n",
    "- **Why do we scale ?**\n",
    "    - To ensure normal distribution of data. Data is often quite wonky.\n",
    "    - To minimise model bias of a particular feature. Ensure that all features are of a similar magnitude.\n",
    "    - Prepare to combine features. Some features may be of very different scales ; scaling allows for them to be combined.\n",
    "    \n",
    "    \n",
    "- **When do we scale ?**\n",
    "    - During the feature engineering step, typically after explore and before modelling.\n",
    "    - This allows for learning from the explore phase, and pick out patters for the model to note.\n",
    "    - Feature engineering includes selecting the best features, those that are most relevant to target variable.\n",
    "    - This occurs AFTER SPLITTING the data.\n",
    "\n",
    "\n",
    "- **How do we scale ?**\n",
    "    - Features are scaled independently.\n",
    "        - If many numerical columns, can we scale them all in one step ? \n",
    "        - We can fit the scalar to many columns of a data set at one time. It learns the parameters of each column independently.\n",
    "    - Uses parameters learnt from the train dataset.\n",
    "\n",
    "\n",
    "- **Linear vs non-linear scalars.**\n",
    "    - Linear preserves the original shape & essence of the data.\n",
    "    - Non-linear changes the shape of the data (ie, distance between some points may be different from the original).\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b39a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9de17d9",
   "metadata": {},
   "source": [
    "## ACQUIRE AND PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a1185",
   "metadata": {},
   "outputs": [],
   "source": [
    "## how to import and read a .data file\n",
    "\n",
    "auto = pd.read_fwf('auto-mpg.data', header = None)\n",
    "\n",
    "# fwf = fixed-width formatted lines\n",
    "\n",
    "# 'header = None' makes it so that row 0 of the dataset is not the column name row.\n",
    "\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2794c61",
   "metadata": {},
   "source": [
    "1. mpg: continuous\n",
    "2. cylinders: multi-valued discrete\n",
    "3. displacement: continuous\n",
    "4. horsepower: continuous\n",
    "5. weight: continuous\n",
    "6. acceleration: continuous\n",
    "7. model year: multi-valued discrete\n",
    "8. origin: multi-valued discrete\n",
    "9. car name: string (unique for each instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.columns = ['mpg', 'cylinders', 'displacement', 'hp', 'weight', \n",
    "                'acceleration', 'model_year', 'origin', 'car_make_model']\n",
    "\n",
    "# make a list of strings from the data dico so that the column names make sense\n",
    "# replaces original column names\n",
    "\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d8bd8",
   "metadata": {},
   "source": [
    "### Remember to handle outliers during exploring and before train-test-split and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a58e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f93a99",
   "metadata": {},
   "source": [
    "**Let's make MPG the target variable : It will not be scaled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bed517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(auto['hp'])\n",
    "\n",
    "# very weird graph. Perhaps some of the data are not int or float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19da9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto[auto['hp'] == '?']\n",
    "\n",
    "# looking at why '.as type' didn't originally work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = auto[auto['hp'] != '?']\n",
    "auto.head()\n",
    "\n",
    "# reassigning variable to where horsepower does not eqyal '?'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b25deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto['hp'] = auto['hp'].astype('float')\n",
    "\n",
    "# this reassigns 'horsepower' column to be a float, permanently. ('inplace = True' did not work in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81b3b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auto.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d3d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a932d408",
   "metadata": {},
   "source": [
    "## MIN–MAX SCALING\n",
    "\n",
    "**Linear scalar : shape of distribution should be unchanged**  \n",
    "**Every value is fit from 0 to 1.**  \n",
    "**Works well with dummies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(auto, train_size = 0.7, random_state = 23)\n",
    "\n",
    "# splitting data, with target variable 'mpg'\n",
    "\n",
    "# can we stratify on a continuous variable ? No, we'd have to bin, \n",
    "  # to make equally-sized groups / categories for each set\n",
    "\n",
    "train.shape, test.shape\n",
    "\n",
    "# normally, we'd create the validate set as well.\n",
    "# this above is being done for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c321c110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the MinMaxScaler to allow for scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d233ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de16951",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scal = MinMaxScaler()\n",
    "\n",
    "# creating an instance of this object within the programme\n",
    "# this scaler is fit ONLY TO THE TRAIN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c4e0f",
   "metadata": {},
   "source": [
    "**Fit one scaler object to all columns at the same time.**  \n",
    "**The object will treat each feature independely.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scal.fit(train[['hp']])\n",
    "\n",
    "# fitting the scaler to 'hp' ——— our target variable 'mpg' is to remain un-scaled\n",
    "\n",
    "# we need the [[]] to avoid an error message : it's a list of a list of the columns\n",
    "# to add more column names, they'd go in the very centre of the [[]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new variable\n",
    "# transforming & making new variable so as not to overwrite original DF data\n",
    "\n",
    "mm_hp = mm_scal.transform(train[['hp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hp'].head()\n",
    "\n",
    "# data BEFORE scaling or transforming : the original, unchanged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de03c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9c31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mm_hp[:5]\n",
    "\n",
    "# looking at the SCALED hp data (cf the results of 'train['hp'].head()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10317180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8807a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(121) # 1 row, 2 columns, the 1st figure\n",
    "\n",
    "sns.displot(train['hp'])\n",
    "\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5b4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a25ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.displot(mm_hp)\n",
    "\n",
    "\n",
    "plt.title('Transformed Data')\n",
    "plt.legend(labels = ['hp', 'sth'])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b64946",
   "metadata": {},
   "source": [
    "## STANDART SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the StandardScaler to allow for scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c41541",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scal = StandardScaler()\n",
    "\n",
    "# creating the std scaler in our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b1c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scal.fit(train[['hp']])\n",
    "\n",
    "# creating the scaler on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88469b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_hp = ss_scal.transform(train[['hp']])\n",
    "ss_hp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7dec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot (121)\n",
    "plt.hist(train['hp'], bins = 25, color = 'pink')\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(ss_hp, bins = 25, color = 'green')\n",
    "plt.title('Transformed Data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b468c75",
   "metadata": {},
   "source": [
    "**We can see that this is linear, because the shape is the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make plots ; we can, in the future, modify the 'original_data'\n",
    "\n",
    "# what will we do with 'transformed_data' ? \n",
    "# Put it in the 2nd plot, 'plt.hist(transformed_data...', in order to compare using different arguments\n",
    "\n",
    "def compare_plots(transformed_data, original_data = train['hp']):\n",
    "    \n",
    "    plt.subplot (121)\n",
    "    plt.hist(original_data, bins = 25, color = 'pink')\n",
    "    plt.title('Original Data')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.hist(transformed_data, bins = 25, color = 'green')\n",
    "    plt.title('Transformed Data')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334f20f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba171622",
   "metadata": {},
   "source": [
    "## ROBUST SCALING\n",
    "\n",
    "**Scaled according to inter-quartile range.**  \n",
    "**Used when outliers are present (if still present).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fbd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the RobustScaler to allow for scaling\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_scal = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a5e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_scal.fit(auto[['hp']])\n",
    "\n",
    "# fitting the scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028579b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_hp = rs_scal.transform(train[['hp']])\n",
    "\n",
    "# transform the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73b3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the above-created function to visualise the data\n",
    "\n",
    "compare_plots(rs_hp)\n",
    "\n",
    "# it appears that the data is LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc960f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## can we fit to multiple columsn at a single time ?\n",
    "\n",
    "rs_scal.fit(train[['hp', 'weight', 'acceleration']])\n",
    "\n",
    "# going forward, create a list object when dealing with multiple variables . \n",
    "# This will ascertain that the list objects are always in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vars = rs_scal.transform(train[['hp', 'weight', 'acceleration']])\n",
    "multi_vars[:10]\n",
    "\n",
    "# transforming the data and saving to a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e269a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_hp[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e64169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning the multiple columns into a DF\n",
    "\n",
    "pd.DataFrame(multi_vars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
